{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5927d27d",
   "metadata": {},
   "source": [
    "# پیاده سازی\n",
    "\n",
    "در قدم اول برای یادگیری شبکه‌های عصبی، یک پیاده سازی ساده از آن را بدون استفاده از کتابخانه‌های رایج انجام می‌دهیم. \n",
    "\n",
    "این پیاده سازی با الهام از کتاب \n",
    "[Neural Networks and Deep Learning](http://neuralnetworksanddeeplearning.com/) \n",
    "انجام شده است. توضیحات بیشتر را می‌توانید در آن کتاب پیدا کنید."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "44a0a113",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "یک ماژول برای پیاده‌سازی الگوریتم یادگیری گرادیان کاهش تصادفی برای یک شبکه عصبی پیش‌خور.\n",
    "گرادیان‌ها با استفاده از روش انتشار معکوس محاسبه می‌شوند.\n",
    "\"\"\"\n",
    "\n",
    "# کتابخانه‌های استاندارد\n",
    "import random\n",
    "\n",
    "# کتابخانه‌های لازم\n",
    "import numpy as np\n",
    "\n",
    "class Network(object):\n",
    "    \"\"\"کلاس شبکه عصبی\"\"\"\n",
    "\n",
    "    def __init__(self, sizes):\n",
    "        \"\"\"مقداردهی اولیه شبکه عصبی\n",
    "        \n",
    "        پارامترها:\n",
    "        sizes -- لیستی که تعداد نورون‌های هر لایه را مشخص می‌کند.\n",
    "                مثال:\n",
    "                [2, 3, 1] برای شبکه‌ای با 2 نورون ورودی،\n",
    "                3 نورون در لایه پنهان و 1 نورون در لایه خروجی\n",
    "        \"\"\"\n",
    "        self.num_layers = len(sizes)\n",
    "        self.sizes = sizes\n",
    "        # مقداردهی تصادفی بایاس‌ها با توزیع نرمال\n",
    "        self.biases = [np.random.randn(y, 1) for y in sizes[1:]]\n",
    "        # مقداردهی تصادفی وزن‌ها با توزیع نرمال\n",
    "        self.weights = [np.random.randn(y, x)\n",
    "                        for x, y in zip(sizes[:-1], sizes[1:])]\n",
    "\n",
    "    def feedforward(self, a , verbose=False):\n",
    "        \"\"\"محاسبه خروجی شبکه برای ورودی داده شده\n",
    "        \n",
    "        پارامترها:\n",
    "        a -- ورودی شبکه\n",
    "        \n",
    "        برگشت:\n",
    "        خروجی شبکه\n",
    "        \"\"\"\n",
    "        for b, w in zip(self.biases, self.weights):\n",
    "            if verbose:\n",
    "                print(\"ورودی لایه:\", a , a.shape)\n",
    "            a = sigmoid( w@a + b)\n",
    "            if verbose:\n",
    "                print(\"وزن‌ها:\", w , w.shape)\n",
    "                print(\"بایاس‌ها:\", b , b.shape)\n",
    "                print(\"خروجی لایه:\", a , a.shape)\n",
    "            \n",
    "        return a\n",
    "\n",
    "    def __call__(self, a):\n",
    "        \"\"\"اجرا کردن شبکه با ورودی داده شده\n",
    "        \n",
    "        پارامترها:\n",
    "        a -- ورودی شبکه\n",
    "        \n",
    "        برگشت:\n",
    "        خروجی شبکه\n",
    "        \"\"\"\n",
    "        return self.feedforward(a , False)\n",
    "    \n",
    "    @property\n",
    "    def nParams(self):\n",
    "        \"\"\"محاسبه تعداد پارامترهای شبکه\n",
    "        \n",
    "        برگشت:\n",
    "        تعداد کل پارامترها (وزن‌ها و بایاس‌ها)\n",
    "        \"\"\"\n",
    "        return sum(w.size for w in self.weights) + sum(b.size for b in self.biases)\n",
    "\n",
    "    def SGD(self, training_data, epochs, mini_batch_size, eta,\n",
    "            test_data=None):\n",
    "        \"\"\"آموزش شبکه با گرادیان کاهش تصادفی روی دسته‌های کوچک\n",
    "        \n",
    "        پارامترها:\n",
    "        training_data -- لیست داده‌های آموزشی به صورت (ورودی, خروجی مطلوب)\n",
    "        epochs -- تعداد دوره‌های آموزشی\n",
    "        mini_batch_size -- اندازه هر دسته کوچک\n",
    "        eta -- نرخ یادگیری\n",
    "        test_data -- داده‌های آزمون (اختیاری)\n",
    "        \"\"\"\n",
    "        if test_data: n_test = len(test_data)\n",
    "        n = len(training_data)\n",
    "        for j in range(epochs):\n",
    "            random.shuffle(training_data)\n",
    "            # ایجاد دسته‌های کوچک\n",
    "            mini_batches = [\n",
    "                training_data[k:k+mini_batch_size]\n",
    "                for k in range(0, n, mini_batch_size)]\n",
    "            # آموزش روی هر دسته کوچک\n",
    "            for mini_batch in mini_batches:\n",
    "                self.update_mini_batch(mini_batch, eta)\n",
    "            # نمایش پیشرفت آموزش\n",
    "            if test_data:\n",
    "                print(\"دوره {0}: {1} / {2}\".format(\n",
    "                    j, self.evaluate(test_data), n_test))\n",
    "            else:\n",
    "                print(\"دوره {0} تکمیل شد\".format(j))\n",
    "\n",
    "    def update_mini_batch(self, mini_batch, eta):\n",
    "        \"\"\"به‌روزرسانی وزن‌ها و بایاس‌ها برای یک دسته کوچک\n",
    "        \n",
    "        پارامترها:\n",
    "        mini_batch -- یک دسته کوچک از داده‌های آموزشی\n",
    "        eta -- نرخ یادگیری\n",
    "        \"\"\"\n",
    "        nabla_b = [np.zeros(b.shape) for b in self.biases]\n",
    "        nabla_w = [np.zeros(w.shape) for w in self.weights]\n",
    "        for x, y in mini_batch:\n",
    "            # محاسبه گرادیان‌ها با انتشار معکوس\n",
    "            delta_nabla_b, delta_nabla_w = self.backprop(x, y)\n",
    "            nabla_b = [nb+dnb for nb, dnb in zip(nabla_b, delta_nabla_b)]\n",
    "            nabla_w = [nw+dnw for nw, dnw in zip(nabla_w, delta_nabla_w)]\n",
    "        # به‌روزرسانی وزن‌ها و بایاس‌ها\n",
    "        self.weights = [w-(eta/len(mini_batch))*nw\n",
    "                        for w, nw in zip(self.weights, nabla_w)]\n",
    "        self.biases = [b-(eta/len(mini_batch))*nb\n",
    "                       for b, nb in zip(self.biases, nabla_b)]\n",
    "\n",
    "    def backprop(self, x, y):\n",
    "        \"\"\"الگوریتم انتشار معکوس برای محاسبه گرادیان‌ها\n",
    "        \n",
    "        پارامترها:\n",
    "        x -- ورودی آموزشی\n",
    "        y -- خروجی مطلوب\n",
    "        \n",
    "        برگشت:\n",
    "        یک تاپل (nabla_b, nabla_w) که گرادیان‌های تابع هزینه را نشان می‌دهد\n",
    "        \"\"\"\n",
    "        nabla_b = [np.zeros(b.shape) for b in self.biases]\n",
    "        nabla_w = [np.zeros(w.shape) for w in self.weights]\n",
    "        # مرحله پیش‌خور\n",
    "        activation = x\n",
    "        activations = [x]  # ذخیره فعال‌سازی‌های هر لایه\n",
    "        zs = []  # ذخیره ورودی‌های هر لایه قبل از اعمال تابع فعال‌سازی\n",
    "        for b, w in zip(self.biases, self.weights):\n",
    "            z = np.dot(w, activation)+b\n",
    "            zs.append(z)\n",
    "            activation = sigmoid(z)\n",
    "            activations.append(activation)\n",
    "        # مرحله پس‌خور\n",
    "        delta = self.cost_derivative(activations[-1], y) * sigmoid_prime(zs[-1])\n",
    "        nabla_b[-1] = delta\n",
    "        nabla_w[-1] = np.dot(delta, activations[-2].transpose())\n",
    "        # محاسبه گرادیان‌ها برای لایه‌های قبلی\n",
    "        for l in range(2, self.num_layers):\n",
    "            z = zs[-l]\n",
    "            sp = sigmoid_prime(z)\n",
    "            delta = np.dot(self.weights[-l+1].transpose(), delta) * sp\n",
    "            nabla_b[-l] = delta\n",
    "            nabla_w[-l] = np.dot(delta, activations[-l-1].transpose())\n",
    "        return (nabla_b, nabla_w)\n",
    "\n",
    "    def evaluate(self, test_data):\n",
    "        \"\"\"ارزیابی عملکرد شبکه روی داده آزمون\n",
    "        \n",
    "        پارامترها:\n",
    "        test_data -- داده‌های آزمون\n",
    "        \n",
    "        برگشت:\n",
    "        تعداد پیش‌بینی‌های صحیح\n",
    "        \"\"\"\n",
    "        #test_results = [ (self.feedforward(x)-y)**2\n",
    "        #                for (x, y) in test_data]\n",
    "        test_results = [(np.argmax(self.feedforward(x)), np.argmax(y) )\n",
    "                        for (x, y) in test_data]\n",
    "        return sum(int(x == y) for (x, y) in test_results)\n",
    "        #return sum(test_results) / len(test_data)\n",
    "    \n",
    "\n",
    "    def cost_derivative(self, output_activations, y):\n",
    "        \"\"\"محاسبه مشتق تابع هزینه\n",
    "        \n",
    "        پارامترها:\n",
    "        output_activations -- فعال‌سازی‌های لایه خروجی\n",
    "        y -- خروجی مطلوب\n",
    "        \n",
    "        برگشت:\n",
    "        مشتق تابع هزینه نسبت به فعال‌سازی‌های خروجی\n",
    "        \"\"\"\n",
    "        return (output_activations-y)\n",
    "\n",
    "# توابع کمکی\n",
    "def sigmoid(z):\n",
    "    \"\"\"تابع فعال‌سازی سیگموئید\"\"\"\n",
    "    return 1.0/(1.0+np.exp(-z))\n",
    "\n",
    "def sigmoid_prime(z):\n",
    "    \"\"\"مشتق تابع سیگموئید\"\"\"\n",
    "    return sigmoid(z)*(1-sigmoid(z))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "0f71c950",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading ../data/mnist.pkl.gz...\n",
      "Downloaded ../data/mnist.pkl.gz\n"
     ]
    }
   ],
   "source": [
    "#A simple code to load and show MNIST dataset\n",
    "\n",
    "#### Libraries\n",
    "# Standard library\n",
    "import pickle\n",
    "import gzip\n",
    "\n",
    "# Third-party libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def download_dataset(url, filename):\n",
    "    \"\"\"\n",
    "    Download the dataset from the given URL if it is not already present.\n",
    "    \"\"\"\n",
    "    import os\n",
    "    import requests\n",
    "\n",
    "    # make sure the directory exists\n",
    "    os.makedirs(os.path.dirname(filename), exist_ok=True)\n",
    "\n",
    "    # Check if the file already exists\n",
    "    # If it does not exist, download it\n",
    "    # If it does exist, do nothing\n",
    "    if not os.path.exists(filename):\n",
    "        print(f\"Downloading {filename}...\")\n",
    "        response = requests.get(url)\n",
    "        with open(filename, 'wb') as f:\n",
    "            f.write(response.content)\n",
    "        print(f\"Downloaded {filename}\")\n",
    "    return True\n",
    "\n",
    "def load_data():\n",
    "    \"\"\"\n",
    "    Load the MNIST dataset from a gzipped pickle file.\n",
    "    \"\"\" \n",
    "\n",
    "    # Download the dataset if it is not already present\n",
    "    dataset_url = 'https://github.com/unexploredtest/neural-networks-and-deep-learning/raw/refs/heads/master/data/mnist.pkl.gz' #'http://deeplearning.net/data/mnist/mnist.pkl.gz'\n",
    "    dataset_filename = '../data/mnist.pkl.gz'\n",
    "    download_dataset(dataset_url, dataset_filename)\n",
    "\n",
    "    f = gzip.open('../data/mnist.pkl.gz', 'rb')\n",
    "    u = pickle._Unpickler(f)\n",
    "    u.encoding = 'latin1'\n",
    "    training_data, validation_data, test_data = u.load()\n",
    "    f.close()\n",
    "    return (training_data, validation_data, test_data)\n",
    "\n",
    "def show_data(data , index=0 , ax=None):\n",
    "    \"\"\"\n",
    "    Show the MNIST data.\n",
    "    \"\"\"\n",
    "    # Extract the first image and label from the training data\n",
    "    image, label = data[0][index], data[1][index] #loads the image and its label\n",
    "\n",
    "    # data is stored as a flat array of 784 pixels (28x28)\n",
    "    # Reshape the image to 28x28 pixels\n",
    "    image = image.reshape(28, 28)\n",
    "\n",
    "    # Display the image\n",
    "    ax.imshow(image, cmap='gray')\n",
    "    ax.set_title(f'Label: {label}')\n",
    "    ax.axis('off')\n",
    "\n",
    "training_data, validation_data, test_data = load_data()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "68b09ce9",
   "metadata": {},
   "outputs": [],
   "source": [
    "yvalues = np.zeros( (50000 , 10))\n",
    "yvalues[range(50000) , training_data[1] ] = 1\n",
    "\n",
    "training_d = list(zip(training_data[0].reshape(-1 , 784 , 1), yvalues.reshape(-1 , 10 , 1)))\n",
    "validation_d = list(zip(validation_data[0].reshape(-1 , 784 , 1), validation_data[1]))\n",
    "\n",
    "yvalues_t = np.zeros( (10000 , 10))\n",
    "yvalues_t[range(10000) , test_data[1] ] = 1\n",
    "\n",
    "test_d = list(zip(test_data[0].reshape(-1 , 784 , 1), yvalues_t.reshape(-1 , 10 , 1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "ec1defe5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "دوره 0: 1049 / 10000\n",
      "دوره 1: 1517 / 10000\n",
      "دوره 2: 1878 / 10000\n",
      "دوره 3: 2299 / 10000\n",
      "دوره 4: 2638 / 10000\n"
     ]
    }
   ],
   "source": [
    "nn = Network([784, 30, 10])\n",
    "nn.SGD(training_d, 5, 1000 , 1,\n",
    "        test_data=test_d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "ee4fb632",
   "metadata": {},
   "outputs": [],
   "source": [
    "xs = np.random.normal( 1 , 1 ,  (10000, 3 , 1) )\n",
    "ys = xs[:,0] + xs[:,1] + xs[:,2]\n",
    "\n",
    "dataset_train = list( zip( xs , ys ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "c46d7095",
   "metadata": {},
   "outputs": [],
   "source": [
    "xs_2 = np.random.normal( 1 , 1 ,  (10000, 3 , 1) )\n",
    "ys_2 = xs_2[:,0] + xs_2[:,1] + xs_2[:,2]\n",
    "\n",
    "dataset_test = list( zip( xs_2 , ys_2 ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "25e90a77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "دوره 0: [[7.51167195]] / 10000\n",
      "دوره 1: [[7.12835131]] / 10000\n",
      "دوره 2: [[7.08896933]] / 10000\n",
      "دوره 3: [[7.07500074]] / 10000\n",
      "دوره 4: [[7.06790705]] / 10000\n",
      "دوره 5: [[7.06363128]] / 10000\n",
      "دوره 6: [[7.06077792]] / 10000\n",
      "دوره 7: [[7.05874127]] / 10000\n",
      "دوره 8: [[7.05721575]] / 10000\n",
      "دوره 9: [[7.0560311]] / 10000\n"
     ]
    }
   ],
   "source": [
    "nn = Network([3 , 2 , 1])\n",
    "nn.SGD(dataset_train , 10 , 1000 , 1.0 , dataset_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "physcomp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
