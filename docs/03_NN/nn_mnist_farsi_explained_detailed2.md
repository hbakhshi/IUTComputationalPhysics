
# ๐ง ุฑุงูููุง ุฌุงูุน ู ุฎุทโุจูโุฎุท ฺฉุฏ ุดุจฺฉูโ ุนุตุจ ุณุงุฏู (MNIST)

ุงู ุฑุงูููุง ุจู ุจุฑุฑุณ ุฏูู ุณุงุฎุชุงุฑ ู ููุทู ฺฉ ุดุจฺฉูโ ุนุตุจ ุณุงุฏู ฺฉู ุจุฑุง ุทุจููโุจูุฏ ุชุตุงูุฑ ูุฌููุนู ุฏุงุฏูโ MNIST ุทุฑุงุญ ุดุฏูุ ูโูพุฑุฏุงุฒุฏ. ูุฑ ุจุฎุด ุงุฒ ฺฉุฏ ุจุง ุชูุถุญุงุช ฺฉุงูู ูุงุฑุณ ููุฑุงู ุงุณุช.

---

## ๐ง ฑ. ุณุงุฎุชุงุฑ ฺฉู ฺฉูุงุณ `Network`

```python
class Network(object):
    def __init__(self, sizes):
        self.num_layers = len(sizes)
        self.sizes = sizes
        self.biases = [np.random.randn(y, 1) for y in sizes[1:]]
        self.weights = [np.random.randn(y, x) for x, y in zip(sizes[:-1], sizes[1:])]
```

### ๐ ุชูุถุญ ุฎุท ุจู ุฎุท:

- `class Network(object):`  
  ุชุนุฑู ฺฉูุงุณ ุดุจฺฉูโ ุนุตุจ. ุงู ฺฉูุงุณ ุดุงูู ูุฒูโูุงุ ุจุงุงุณโูุง ู ุชูุงุจุน ุขููุฒุด ุงุณุช.

- `def __init__(self, sizes):`  
  ุชุงุจุน ุณุงุฒูุฏู ฺฉู ุขุฑุงูโุง ุจู ูุงู `sizes` ูโฺฏุฑุฏุ ุดุงูู ุชุนุฏุงุฏ ููุฑููโูุง ุฏุฑ ูุฑ ูุงู.

- `self.num_layers = len(sizes)`  
  ุชุนู ุชุนุฏุงุฏ ูุงูโูุง ุจุฑ ุงุณุงุณ ุทูู ุขุฑุงูโ `sizes`.

- `self.sizes = sizes`  
  ุฐุฎุฑู ูุณุช ุณุงุฎุชุงุฑ ูุงูโูุง ุฏุฑ ูฺฺฏ `sizes`.

- `self.biases = [np.random.randn(y, 1) for y in sizes[1:]]`  
  ููุฏุงุฑุฏู ุงููู ุจุงุงุณโูุง ุจุฑุง ุชูุงู ูุงูโูุง ุบุฑ ุงุฒ ูุฑูุฏ.  
  ุจุงุงุณ ูุฑ ููุฑูู ุงุฒ ุชูุฒุน ูุฑูุงู ุงุณุชุงูุฏุงุฑุฏ (`mean=0, std=1`) ููุฏุงุฑุฏู ูโุดูุฏ.

- `self.weights = [np.random.randn(y, x) for x, y in zip(sizes[:-1], sizes[1:])]`  
  ููุฏุงุฑุฏู ุงููู ูุฒูโูุง ุจู ูุฑ ุฏู ูุงูโ ูุชูุงู.  
  ูุฒูโูุง ูุงุชุฑุณโูุง ุจุง ุงุจุนุงุฏ `(ุชุนุฏุงุฏ ููุฑููโูุง ูุงูโ ูุนู ร ุชุนุฏุงุฏ ููุฑููโูุง ูุงูโ ูุจู)` ูุณุชูุฏ.

---

## ๐งฎ ฒ. ูุญุงุณุจู ุชุนุฏุงุฏ ูพุงุฑุงูุชุฑูุง

```python
@property
def nParams(self):
    return sum(w.size for w in self.weights) + sum(b.size for b in self.biases)
```

### ๐ ุชูุถุญ:

- ุงู ูฺฺฏ ุชุนุฏุงุฏ ฺฉู ูพุงุฑุงูุชุฑูุง ูุงุจู ุงุฏฺฏุฑ (ูุฒู + ุจุงุงุณ) ุฏุฑ ุดุจฺฉู ุฑุง ุจุงุฒูโฺฏุฑุฏุงูุฏ.
- `w.size` ู `b.size` ุชุนุฏุงุฏ ุนูุงุตุฑ ูุฑ ูุฒู ู ุจุงุงุณ ุฑุง ูุญุงุณุจู ูโฺฉูุฏ.

---

## ๐ ณ. ูพุดโุฎูุฑ (Feedforward)

```python
def feedforward(self, a, verbose=False):
    for b, w in zip(self.biases, self.weights):
        if verbose:
            print("ูุฒู:", w.shape, "ุจุงุงุณ:", b.shape)
        a = sigmoid(w @ a + b)
    return a
```

### ๐ ุชูุถุญ:

- `a` ูุฑูุฏ ุงููู ุดุจฺฉู (ุชุตูุฑ ูุณุทุญ ุดุฏู) ุงุณุช.
- ุฏุฑ ูุฑ ูุงู:
  - ุงุจุชุฏุง `z = w @ a + b` ูุญุงุณุจู ูโุดูุฏ.
  - ุณูพุณ ุณฺฏููุฆุฏ ุฑู ุขู ุงุนูุงู ูโุดูุฏ.
  - `a` ุจุฑุง ูุงู ุจุนุฏ ุจูโุฑูุฒ ูโุดูุฏ.

---

## ๐ ด. ุงูุชุดุงุฑ ูุนฺฉูุณ (Backpropagation)

```python
def backprop(self, x, y):
    activation = x
    activations = [x]
    zs = []
    for b, w in zip(self.biases, self.weights):
        z = w.dot(activation) + b
        zs.append(z)
        activation = sigmoid(z)
        activations.append(activation)

    delta = self.cost_derivative(activations[-1], y) * sigmoid_prime(zs[-1])
    nabla_b[-1] = delta
    nabla_w[-1] = np.dot(delta, activations[-2].T)
    for l in range(2, self.num_layers):
        z = zs[-l]
        sp = sigmoid_prime(z)
        delta = np.dot(self.weights[-l+1].T, delta) * sp
        nabla_b[-l] = delta
        nabla_w[-l] = np.dot(delta, activations[-l-1].T)
    return (nabla_b, nabla_w)
```

### ๐ ุชูุถุญ:

- ูุฑุญูู feedforward ุฑุง ุชฺฉุฑุงุฑ ูโฺฉูุฏ ู `z` ู `activation`โูุง ุฑุง ุฐุฎุฑู ูโฺฉูุฏ.
- ุฏุฑ ูุฑุญูู ุจุฑฺฏุดุช:
  - ฺฏุฑุงุฏุงู ุชุงุจุน ูุฒูู ูุญุงุณุจู ูโุดูุฏ.
  - ุณูพุณ ฺฏุฑุงุฏุงูโูุง ุจุฑุง ุชูุงู ูุฒูโูุง ู ุจุงุงุณโูุง ุฏุฑ ูุฑ ูุงู ุจูโุฏุณุช ูโุขุฏ.
  - ุงุฒ ูุดุชู ุชุงุจุน ุณฺฏููุฆุฏ ุงุณุชูุงุฏู ูโุดูุฏ.

---

## ๐งช ต. ุจูโุฑูุฒุฑุณุงู ูุฒูโูุง ุจุง Mini-Batch

```python
def update_mini_batch(self, mini_batch, eta):
    for x, y in mini_batch:
        delta_b, delta_w = self.backprop(x, y)
        nabla_b = [nb + db for nb, db in zip(nabla_b, delta_b)]
        nabla_w = [nw + dw for nw, dw in zip(nabla_w, delta_w)]
    self.weights = [w - (eta / len(mini_batch)) * nw for w, nw in zip(self.weights, nabla_w)]
    self.biases = [b - (eta / len(mini_batch)) * nb for b, nb in zip(self.biases, nabla_b)]
```

### ๐ ุชูุถุญ:

- ุจุฑุง ูุฑ mini-batch ุงุฒ ุฏุงุฏูโูุง ุขููุฒุด:
  - ฺฏุฑุงุฏุงูโูุง ูุญุงุณุจู ูโุดูุฏ.
  - ูุฒูโูุง ู ุจุงุงุณโูุง ุจุง ูุงูฺฏู ฺฏุฑุงุฏุงูโูุง ู ูุฑุฎ ุงุฏฺฏุฑ `eta` ุจูโุฑูุฒุฑุณุงู ูโุดููุฏ.

---

## ๐ข ถ. ุชูุงุจุน ุณฺฏููุฆุฏ ู ูุดุชู ุขู

```python
def sigmoid(z): return 1.0 / (1.0 + np.exp(-z))
def sigmoid_prime(z): return sigmoid(z) * (1 - sigmoid(z))
```

### ๐ ุชูุถุญ:

- ุณฺฏููุฆุฏ ุชุงุจุน ูุนุงูโุณุงุฒ ุงุณุชุงูุฏุงุฑุฏ ุจุฑุง ููุฑููโูุงุณุช.
- ูุดุชู ุขู ุจุฑุง backpropagation ุถุฑูุฑ ุงุณุช.

---

## ๐ ท. ุจุงุฑฺฏุฐุงุฑ ุฏุงุฏูโูุง MNIST

```python
training_data, validation_data, test_data = load_data()
yvalues = np.zeros((50000, 10))
yvalues[np.arange(50000), training_data[1]] = 1
training_d = list(zip(...))
```

### ๐ ุชูุถุญ:

- ุฏุงุฏูโูุง ุดุงูู ุชุตุงูุฑ ู ุจุฑฺุณุจโูุง ุงุณุช.
- ุจุฑฺุณุจโูุง ุจู ุตูุฑุช one-hot encoded ุชุจุฏู ูโุดููุฏ ุชุง ุจุฑุง ุฎุฑูุฌ softmax ุขูุงุฏู ุจุงุดูุฏ.

---

## โถ๏ธ ธ. ุขููุฒุด ูุฏู

```python
nn.SGD(training_d, epochs=5, mini_batch_size=1000, eta=1.0, test_data=test_d)
```

### ๐ ุชูุถุญ:

- ุดุจฺฉู ุจุง ุงุณุชูุงุฏู ุงุฒ ุงูฺฏูุฑุชู SGD ุขููุฒุด ุฏุงุฏู ูโุดูุฏ.
- ุฏุฑ ูุฑ ุฏูุฑู (epoch) ูุฒูโูุง ุจุง ุงุณุชูุงุฏู ุงุฒ mini-batchูุง ุงุตูุงุญ ูโุดููุฏ.

---

## ๐ง น. ุงุณุชูุงุฏู ุฏุฑ HTML

```html
<details>
  <summary>ุชูุถุญุงุช ฺฉุงูู ุดุจฺฉู</summary>

  <!-- ุงูุฌุง ูุชู ูุงุฑฺฉโุฏุงูู ูุฑุงุฑ ฺฏุฑุฏ -->
</details>
```

---


---

## ๐ ุชูุถุญ ููุตู ุชุงุจุน `update_mini_batch`

```python
def update_mini_batch(self, mini_batch, eta):
    for x, y in mini_batch:
        delta_b, delta_w = self.backprop(x, y)
        nabla_b = [nb + db for nb, db in zip(nabla_b, delta_b)]
        nabla_w = [nw + dw for nw, dw in zip(nabla_w, delta_w)]
    self.weights = [w - (eta / len(mini_batch)) * nw for w, nw in zip(self.weights, nabla_w)]
    self.biases = [b - (eta / len(mini_batch)) * nb for b, nb in zip(self.biases, nabla_b)]
```

### ๐งบ ููููู Batch ู Mini-Batch

ุฏุฑ ุขููุฒุด ุดุจฺฉูโูุง ุนุตุจุ ุฏุงุฏูโูุง ูุนูููุงู ุฏุฑ ุจุณุชูโูุง (batchูุง) ุจู ุดุจฺฉู ุฏุงุฏู ูโุดููุฏ:

- **Batch**: ฺฉู ูุฌููุนูโ ุฏุงุฏูโูุง ฺฉโุจุงุฑู ุจู ุดุจฺฉู ุฏุงุฏู ูโุดูุฏ.
- **Mini-Batch**: ูุฌููุนูโ ุฏุงุฏู ุจู ูุณูุชโูุง ฺฉูฺฺฉุชุฑ ุชูุณู ุดุฏู ู ูุฑ ุจุงุฑ ููุท ฺฉ ูุณูุช (ูุซูุงู ฑฐฐ ุฏุงุฏู) ุจู ุดุจฺฉู ุฏุงุฏู ูโุดูุฏ.  
  ุงู ุฑูุด ุจุงุนุซ ุงูุฒุงุด ูพุงุฏุงุฑ ู ุณุฑุนุช ุงุฏฺฏุฑ ูโุดูุฏ.

ุฏุฑ ุงู ุชุงุจุนุ `mini_batch` ฺฉ ูุณุช ุงุฒ ุฒูุฌโูุง `(x, y)` ุงุณุช ฺฉู ุฏุฑ ูุฑ epoch ุงูุชุฎุงุจ ูโุดููุฏ.

---

### โ ููููู ูุดุชู ุฏุฑ ุงุฏฺฏุฑ ูุงุดู

ูุดุชู ุชุงุจุน ูุฒูู ูุณุจุช ุจู ูพุงุฑุงูุชุฑูุง ุดุจฺฉู (ูุฒู ู ุจุงุงุณ)ุ ูุดุฎุต ูโฺฉูุฏ ฺฉู ุชุบุฑ ุขู ูพุงุฑุงูุชุฑ ฺู ุงุซุฑ ุจุฑ ุฑู ููุฏุงุฑ ุฎุทุง ุฏุงุฑุฏ.

- ูุดุชู ุชุงุจุน ูุฒูู (Cost function) = ูุฑุฎ ุชุบุฑ ุฎุทุง ูุณุจุช ุจู ูุฒูโูุง ุง ุจุงุงุณโูุง
- ูุฏู ุงูฺฏูุฑุชู ฺฏุฑุงุฏุงู ฺฉุงูุด (Gradient Descent) ฺฉููู ฺฉุฑุฏู ุชุงุจุน ูุฒูู ุงุณุช.

ูุฑููู ุงุตู ุจูโุฑูุฒุฑุณุงู ูุฒูโูุง:

\[
w = w - \eta \cdot rac{\partial C}{\partial w}
\]

ุฏุฑ ุงูุฌุง:
- \( \eta \) ูุฑุฎ ุงุฏฺฏุฑ (learning rate) ุงุณุช.
- \( rac{\partial C}{\partial w} \) ฺฏุฑุงุฏุงู ุชุงุจุน ูุฒูู ูุณุจุช ุจู ูุฒู \( w \) ุงุณุช.

---

### ๐ ูพุงุฏูโุณุงุฒ ุฏุฑ ฺฉุฏ

- `self.backprop(x, y)` ฺฏุฑุงุฏุงู ูุฒูโูุง ู ุจุงุงุณโูุง ุฑุง ุจุฑุง ฺฉ ููููู (x, y) ูุญุงุณุจู ูโฺฉูุฏ.
- ุณูพุณ `delta_b` ู `delta_w` ฺฉู ุฎุฑูุฌโูุง `backprop` ูุณุชูุฏุ ุจุง `nabla_b` ู `nabla_w` ุฌูุน ูโุดููุฏ ุชุง ูุฌููุน ฺฏุฑุงุฏุงูโูุง ุจุฑุง ฺฉู mini-batch ุจุฏุณุช ุขุฏ.
- ุฏุฑ ููุงุชุ ูุฒูโูุง ู ุจุงุงุณโูุง ุจุง ูุงูฺฏู ฺฏุฑุงุฏุงูโูุง (ุชูุณู ุจุฑ ุงูุฏุงุฒูโ mini_batch) ุจูโุฑูุฒุฑุณุงู ูโุดููุฏ.

---

## ๐ ุชูุถุญ ููุตู ุชุงุจุน `backprop`

```python
def backprop(self, x, y):
    activation = x
    activations = [x]
    zs = []
    for b, w in zip(self.biases, self.weights):
        z = w.dot(activation) + b
        zs.append(z)
        activation = sigmoid(z)
        activations.append(activation)

    delta = self.cost_derivative(activations[-1], y) * sigmoid_prime(zs[-1])
    nabla_b[-1] = delta
    nabla_w[-1] = np.dot(delta, activations[-2].T)
    for l in range(2, self.num_layers):
        z = zs[-l]
        sp = sigmoid_prime(z)
        delta = np.dot(self.weights[-l+1].T, delta) * sp
        nabla_b[-l] = delta
        nabla_w[-l] = np.dot(delta, activations[-l-1].T)
    return (nabla_b, nabla_w)
```

---

### ๐ง ููุด ุงู ุชุงุจุน ฺุณุชุ

ุงู ุชุงุจุน ูุณุฆูู ุงุฌุฑุง **ุงูุชุดุงุฑ ูุนฺฉูุณ (Backpropagation)** ุงุณุช โ ููุจ ุงูฺฏูุฑุชู ุงุฏฺฏุฑ ุดุจฺฉูโูุง ุนุตุจ!

ุฏุฑ ุท ุงู ูุฑุขูุฏุ ฺฏุฑุงุฏุงู ุชุงุจุน ูุฒูู ูุณุจุช ุจู ูพุงุฑุงูุชุฑูุง (ูุฒูโูุง ู ุจุงุงุณโูุง) ูุญุงุณุจู ูโุดูุฏ.

---

### ูุฑุงุญู ุงุฌุฑุง `backprop`:

1. **ูุฑุญููโ forward:**
   - ุดุจฺฉู ฺฉ ูุฑูุฏ `x` ุฑุง ุฏุฑุงูุช ฺฉุฑุฏู ู ุจู ุชุฑุชุจ ุงุฒ ูุงูโูุง ุนุจูุฑ ูโุฏูุฏ.
   - `z`ูุง (ุฎุฑูุฌ ูุจู ุงุฒ ุชุงุจุน ูุนุงูโุณุงุฒ) ู `activation`ูุง (ุฎุฑูุฌ ูพุณ ุงุฒ ุชุงุจุน ูุนุงูโุณุงุฒ) ุจุฑุง ูุฑ ูุงู ุฐุฎุฑู ูโุดููุฏ.

2. **ูุญุงุณุจู ุฎุทุง ูุงู ุฎุฑูุฌ (delta):**
   - ุจุฑุง ูุงูโ ุขุฎุฑ:
     \[
     \delta = 
abla_a C \cdot \sigma'(z)
     \]
     ฺฉู ุฏุฑ ฺฉุฏ ุขูุฏู:
     ```python
     delta = self.cost_derivative(activations[-1], y) * sigmoid_prime(zs[-1])
     ```

3. **ุงูุชูุงู ุฎุทุง ุจู ูุงูโูุง ูุจู:**
   - ุจุฑุง ูุฑ ูุงูโ ูุจูุ delta ุฌุฏุฏ ุจุง ุงุณุชูุงุฏู ุงุฒ ูุฒูโูุง ูุงู ุจุนุฏ ู ูุดุชู ุณฺฏููุฆุฏ ูุญุงุณุจู ูโุดูุฏ:
     ```python
     delta = np.dot(self.weights[-l+1].T, delta) * sp
     ```

4. **ูุญุงุณุจู ฺฏุฑุงุฏุงู ูุฒู ู ุจุงุงุณ:**
   - ุจุงุงุณโูุง: ููุงู delta ูุณุชูุฏ.
   - ูุฒูโูุง: ุถุฑุจ delta ุฏุฑ transposed activation ูุงูโ ูุจู ุงุฒ ุขู.

---

### โ๏ธ ุชุงุจุน `cost_derivative`

```python
def cost_derivative(self, output_activations, y):
    return (output_activations - y)
```

- ฺฏุฑุงุฏุงู ุชุงุจุน ูุฒูู ูุณุจุช ุจู ุฎุฑูุฌ ุดุจฺฉู:
  \[
  rac{\partial C}{\partial a} = a - y
  \]

- ูุฑุถ ุดุฏู ฺฉู ุชุงุจุน ูุฒูู ุงุฒ ููุน **Mean Squared Error** ุงุณุช.

---

### ๐งฉ ุฌูุนโุจูุฏ

- `backprop`: ูุญุงุณุจู ูุดุชูุงุช (ฺฏุฑุงุฏุงูโูุง)
- `update_mini_batch`: ุจูโุฑูุฒุฑุณุงู ูุฒูโูุง ุจุง ูุงูฺฏู ฺฏุฑุงุฏุงูโูุง ฺฉ mini-batch
- ููููู ูุดุชู ู ฺฏุฑุงุฏุงู ุฏุฑ ุงุฏฺฏุฑ ุจุณุงุฑ ฺฉูุฏ ุงุณุช: ูุณุฑ ุจููู ุฑุง ุจุฑุง ฺฉุงูุด ุฎุทุง ูุดุฎุต ูโฺฉูุฏ.
- mini-batch ุจุงุนุซ ุชุนุงุฏู ูุงู ฺฉุงุฑุง ูุญุงุณุจุงุช ู ูููู (noise) ุฏุฑ ฺฏุฑุงุฏุงู ูโุดูุฏ.

---
