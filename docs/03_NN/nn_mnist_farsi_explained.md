
# ğŸ§  Ø±Ø§Ù‡Ù†Ù…Ø§ÛŒ Ú¯Ø§Ù…â€ŒØ¨Ù‡â€ŒÚ¯Ø§Ù… Ú©Ø¯ Ø´Ø¨Ú©Ù‡â€ŒÛŒ Ø¹ØµØ¨ÛŒ (MNIST) Ø¯Ø± Ù¾Ø§ÛŒØªÙˆÙ†

## ğŸ” Û±. Ø§ÛŒØ¯Ù‡â€ŒÛŒ Ú©Ù„ÛŒ

- Ø§ÛŒÙ† Ø¯Ø±Ø³ØŒ Ù¾ÛŒØ§Ø¯Ù‡â€ŒØ³Ø§Ø²ÛŒ Ø³Ø§Ø¯Ù‡â€ŒØ§ÛŒ Ø§Ø² ÛŒÚ© **Ø´Ø¨Ú©Ù‡â€ŒÛŒ Ø¹ØµØ¨ÛŒ Ù¾ÛŒØ´â€ŒØ®ÙˆØ±** (feedâ€‘forward) Ø±Ø§ Ø¨Ø§ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² Ø¯Ø³ØªÙˆØ±Ø§Øª Ù¾Ø§ÛŒÙ‡â€ŒÛŒ Ù¾Ø§ÛŒØªÙˆÙ† Ùˆ NumPy Ø§Ø±Ø§Ø¦Ù‡ Ù…ÛŒâ€ŒØ¯Ù‡Ø¯.
- Ù‡Ø¯Ù Ø§ØµÙ„ÛŒ: Ø·Ø¨Ù‚Ù‡â€ŒØ¨Ù†Ø¯ÛŒ ØªØµØ§ÙˆÛŒØ± Ø¯Ø³Øªâ€ŒÙ†ÙˆÛŒØ³ MNISTØŒ ÛŒØ¹Ù†ÛŒ Û±Û° Ú©Ù„Ø§Ø³ Ø±Ù‚Ù…ÛŒØŒ Ø¨Ø¯ÙˆÙ† ÙˆØ§Ø¨Ø³ØªÚ¯ÛŒ Ø¨Ù‡ ÙØ±ÛŒÙ…â€ŒÙˆØ±Ú©â€ŒÙ‡Ø§ÛŒ Ø­Ø¬ÛŒÙ….
- Ù¾ÛŒØ§Ø¯Ù‡â€ŒØ³Ø§Ø²ÛŒ Ø¨Ø§ Ø§Ù„Ù‡Ø§Ù… Ø§Ø² Ú©ØªØ§Ø¨ *Neural Networks and Deep Learning* Ø§Ù†Ø¬Ø§Ù… Ø´Ø¯Ù‡ Ø§Ø³Øª.

---

## ğŸ§± Û². Ø³Ø§Ø®ØªØ§Ø± Ú©Ù„Ø§Ø³ `Network`

```python
class Network(object):
    def __init__(self, sizes):
        self.num_layers = len(sizes)
        self.sizes = sizes
        self.biases = [np.random.randn(y, 1) for y in sizes[1:]]
        self.weights = [np.random.randn(y, x) for x, y in zip(sizes[:-1], sizes[1:])]
```

- Ù„ÛŒØ³Øª `sizes` Ø¹Ø±Ø¶ ÙˆØ±ÙˆØ¯ÛŒØŒ ØªØ¹Ø¯Ø§Ø¯ Ù†ÙˆØ±ÙˆÙ†â€ŒÙ‡Ø§ÛŒ Ù„Ø§ÛŒÙ‡â€ŒÙ‡Ø§ÛŒ Ù¾Ù†Ù‡Ø§Ù†ØŒ Ùˆ Ø®Ø±ÙˆØ¬ÛŒâ€ŒÙ‡Ø§ Ø±Ø§ ØªØ¹ÛŒÛŒÙ† Ù…ÛŒâ€ŒÚ©Ù†Ø¯.  
  Ù…Ø«Ø§Ù„: `[784, 30, 10]` Ø¨Ø±Ø§ÛŒ MNIST.
- `biases` Ùˆ `weights` Ø¨Ø§ Ù†Ù…ÙˆÙ†Ù‡â€ŒÚ¯ÛŒØ±ÛŒ Ø§Ø² ØªÙˆØ²ÛŒØ¹ Ù†Ø±Ù…Ø§Ù„ Ù…Ù‚Ø¯Ø§Ø±Ø¯Ù‡ÛŒ Ø§ÙˆÙ„ÛŒÙ‡ Ù…ÛŒâ€ŒØ´ÙˆÙ†Ø¯.

### ÙˆÛŒÚ˜Ú¯ÛŒ `nParams`

```python
@property
def nParams(self):
    return sum(w.size for w in self.weights) + sum(b.size for b in self.biases)
```

---

## ğŸš€ Û³. Ù¾ÛŒØ´â€ŒØ®ÙˆØ± (Feedforward)

```python
def feedforward(self, a, verbose=False):
    for b, w in zip(self.biases, self.weights):
        if verbose:
            print("ÙˆØ²Ù†:", w.shape, "Ø¨Ø§ÛŒØ§Ø³:", b.shape)
        a = sigmoid(w @ a + b)
    return a
```

---

## ğŸ”„ Û´. Ø§Ù†ØªØ´Ø§Ø± Ù…Ø¹Ú©ÙˆØ³ (Backpropagation)

```python
def backprop(self, x, y):
    activation = x
    activations = [x]
    zs = []
    for b, w in zip(self.biases, self.weights):
        z = w.dot(activation) + b
        zs.append(z)
        activation = sigmoid(z)
        activations.append(activation)

    delta = self.cost_derivative(activations[-1], y) * sigmoid_prime(zs[-1])
    nabla_b[-1] = delta
    nabla_w[-1] = np.dot(delta, activations[-2].T)
    for l in range(2, self.num_layers):
        z = zs[-l]
        sp = sigmoid_prime(z)
        delta = np.dot(self.weights[-l+1].T, delta) * sp
        nabla_b[-l] = delta
        nabla_w[-l] = np.dot(delta, activations[-l-1].T)
    return (nabla_b, nabla_w)
```

---

## ğŸ§ª Ûµ. Ø¢Ù…ÙˆØ²Ø´ Ø¨Ø§ Ú¯Ø±Ø§Ø¯ÛŒØ§Ù† Ú©Ø§Ù‡Ø´ ØªØµØ§Ø¯ÙÛŒ (SGD)

```python
def update_mini_batch(self, mini_batch, eta):
    for x, y in mini_batch:
        delta_b, delta_w = self.backprop(x, y)
        nabla_b = [nb + db for nb, db in zip(nabla_b, delta_b)]
        nabla_w = [nw + dw for nw, dw in zip(nabla_w, delta_w)]
    self.weights = [w - (eta / len(mini_batch)) * nw for w, nw in zip(self.weights, nabla_w)]
    self.biases = [b - (eta / len(mini_batch)) * nb for b, nb in zip(self.biases, nabla_b)]
```

---

## ğŸ“¦ Û¶. ØªÙˆØ§Ø¨Ø¹ Ø³ÛŒÚ¯Ù…ÙˆØ¦ÛŒØ¯ Ùˆ Ù…Ø´ØªÙ‚ Ø¢Ù†

```python
def sigmoid(z): return 1.0 / (1.0 + np.exp(-z))
def sigmoid_prime(z): return sigmoid(z) * (1 - sigmoid(z))
```

---

## ğŸ“‚ Û·. Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ùˆ Ù†Ù…Ø§ÛŒØ´ Ø¯Ø§Ø¯Ù‡â€ŒÛŒ MNIST

```python
training_data, validation_data, test_data = load_data()
yvalues = np.zeros((50000, 10))
yvalues[np.arange(50000), training_data[1]] = 1
training_d = list(zip(...))
test_d = list(zip(...))
```

---

## ğŸ“Š Û¸. Ù…Ø«Ø§Ù„ Ø§Ø¬Ø±Ø§ Ùˆ Ù†ØªØ§ÛŒØ¬

```python
nn.SGD(training_d, epochs=5, mini_batch_size=1000, eta=1.0, test_data=test_d)
```

---

## ğŸ”§ Û±Û°. Ù†Ø­ÙˆÙ‡â€ŒÛŒ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø¯Ø± ØµÙØ­Ù‡â€ŒÛŒ ÙˆØ¨

```html
<details>
  <summary>ØªÙˆØ¶ÛŒØ­Ø§Øª Ú©Ø§Ù…Ù„ Ø´Ø¨Ú©Ù‡</summary>

  <!-- Ø§ÛŒÙ†Ø¬Ø§ Ù…ØªÙ† Ù…Ø§Ø±Ú©â€ŒØ¯Ø§ÙˆÙ† Ù‚Ø±Ø§Ø± Ú¯ÛŒØ±Ø¯ -->
</details>
```

---
